{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":555,"status":"ok","timestamp":1708933291477,"user":{"displayName":"Hosein Nekouei","userId":"03847242090053515103"},"user_tz":-210},"id":"_BVCp1kka2J0","outputId":"1f1bb285-b30a-4d10-d35e-a0cc7ee982dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Feb 26 07:41:31 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17293,"status":"ok","timestamp":1708933311175,"user":{"displayName":"Hosein Nekouei","userId":"03847242090053515103"},"user_tz":-210},"id":"yukH1qFmaEQO","outputId":"fbec63b4-0ee1-4f77-8e33-e77fef1d878d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","# Review the uploaded file and provided detail information\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"NC0i_WUAZZIB","executionInfo":{"status":"ok","timestamp":1708933319253,"user_tz":-210,"elapsed":6582,"user":{"displayName":"Hosein Nekouei","userId":"03847242090053515103"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import Model\n","from keras import layers\n","from keras.utils import text_dataset_from_directory"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"EKKkfNPhZjhs","executionInfo":{"status":"ok","timestamp":1708933319254,"user_tz":-210,"elapsed":17,"user":{"displayName":"Hosein Nekouei","userId":"03847242090053515103"}}},"outputs":[],"source":["BATCH_SIZE = 32\n","MAX_TOKENS = 5000\n","MAX_LENGTH = 142"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qGk2mC4lZmAc","executionInfo":{"status":"ok","timestamp":1708933319254,"user_tz":-210,"elapsed":16,"user":{"displayName":"Hosein Nekouei","userId":"03847242090053515103"}}},"outputs":[],"source":["def load_text_dataset(directory_path: str, batch_size: int):\n","    '''\n","        Load a text dataset from the specified directory.\n","\n","        Args:\n","            directory_path (str):\n","                Path to the directory containing the text data.\n","\n","            batch_size(int):\n","                Number of samples per batch.\n","\n","        Return:\n","            tf.data.Dataset:\n","            TensorFlow dataset containing the text data.\n","    '''\n","    dataset = text_dataset_from_directory(\n","                    directory_path, batch_size= batch_size)\n","\n","    # print the shape and datatype data (for data accuracy control)\n","    for inputs, targets in dataset:\n","        print(f'Shape of inputs: {inputs.shape}')\n","        print(f'dtype of inputs: {inputs.dtype}')\n","        print(f'taeger[0]: {targets[0]}')\n","        break\n","\n","    return dataset\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"B_cIHe8BZtAW","executionInfo":{"status":"ok","timestamp":1708933321090,"user_tz":-210,"elapsed":2,"user":{"displayName":"Hosein Nekouei","userId":"03847242090053515103"}}},"outputs":[],"source":["def proprocess_text_data(train_ds: tf.data.Dataset,test_ds: tf.data.Dataset,\n","                         max_token: int, output_mode: str, output_sequence_length: int) -> tuple[tf.data.Dataset, tf.data.Dataset]:\n","    '''\n","        Loads and preprocesses text data from the specified datasets.\n","\n","        Args:\n","            Train_ds:\n","                Training dataset containing text samples.\n","\n","            Test_ds:\n","                Test dataset containing text samples.\n","\n","            Max_token (int):\n","                Maximum number of tokens in the vocabulary.\n","\n","            Output_mode (str):\n","                Output mode ('int' for integer-encoded tokens).\n","\n","            Output_sequence_length (int):\n","                Maximum sequence length for the output (length of the dictionary).\n","\n","        Returns:\n","            tf.data.Dataset:\n","                proccessed training and test datasets\n","    '''\n","    # Create a TextVectorization layer\n","    vectorizer = layers.TextVectorization(max_tokens= max_token,\n","                                                  output_mode= output_mode,\n","                                                  output_sequence_length= output_sequence_length)\n","\n","    # Adapt the TextVectorization layer to the text data\n","    vectorizer.adapt(train_ds.unbatch().map(lambda x,y: x).batch(BATCH_SIZE))\n","\n","    # Apply the adapted TextVectorization layer to both training and test datasets\n","    int_train_ds = train_ds.map(lambda x, y: (vectorizer(x), y), num_parallel_calls=4)\n","    int_test_ds = test_ds.map(lambda x, y: (vectorizer(x), y), num_parallel_calls=4)\n","\n","    # print the top token in the vocabbulary (for data accuracy control)\n","    print(vectorizer.get_vocabulary()[:10])\n","\n","    return (int_train_ds, int_test_ds)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eq5CDCEjZ0X_","executionInfo":{"status":"ok","timestamp":1708933321980,"user_tz":-210,"elapsed":3,"user":{"displayName":"Hosein Nekouei","userId":"03847242090053515103"}}},"outputs":[],"source":["def gru_model(max_token: int)-> keras.Model:\n","    '''\n","        Create an gru_based binary classification model.\n","\n","        Args:\n","            max_token (int):\n","                Maximun number of tokens in the vocabulary.\n","\n","        returns:\n","            keras.Model:\n","                A compiled LSTM model for binary classification.\n","    '''\n","    # Define input layer\n","    inputs = keras.Input(shape=(None,), dtype ='int64')\n","\n","    # Embedding layer\n","    embeded= layers.Embedding(input_dim= max_token, output_dim= 128)(inputs)\n","\n","    # Create the GRU model using the functional API\n","    x = layers.GRU(units= 64, recurrent_dropout=0.5, return_sequences=True)(embeded) # Increased hidden state size\n","    x = layers.GRU(units= 64, recurrent_dropout=0.5)(x)\n","    x = layers.Dropout(0.5)(x)\n","     # Increased hidden state size\n","\n","    # Output layer\n","    outputs = layers.Dense(1, activation= 'sigmoid')(x)\n","\n","    # Create model\n","    model = Model(inputs, outputs)\n","\n","    # Compile the model\n","    model.compile(optimizer=\"adam\",\n","                loss=\"binary_crossentropy\",\n","                metrics=[\"accuracy\"])\n","\n","    model.summary()\n","\n","    history = model.fit(int_train_ds, validation_data= int_test_ds, epochs = 10)\n","\n","    return history"]},{"cell_type":"code","source":["def show_results(history):\n","    plt.style.use('ggplot')\n","\n","    # Get the loss and accuracy values from the history object\n","    loss = history.history['loss']\n","    val_loss = history.history['val_loss']\n","    accuracy = history.history['accuracy']\n","    val_accuracy = history.history['val_accuracy']\n","\n","    # Plot the loss and accuracy curves\n","    plt.plot(np.arange(EPOCHS), loss, label='Train Loss')\n","    plt.plot(np.arange(EPOCHS), val_loss, label='Validation Loss')\n","\n","    plt.plot(np.arange(EPOCHS), accuracy, label='Accuracy')\n","    plt.plot(np.arange(EPOCHS), val_accuracy, label='Validation Accuracy')\n","\n","    plt.legend()\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss/Accuracy')\n","    plt.title('IMDB segmentation training')\n","\n","    plt.show()\n"],"metadata":{"id":"Hq2c_gNna5b8","executionInfo":{"status":"ok","timestamp":1708933323339,"user_tz":-210,"elapsed":3,"user":{"displayName":"Hosein Nekouei","userId":"03847242090053515103"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JuwqZ6cA4j2q","outputId":"b92959e1-59aa-440b-8c70-093b2fe235ac","executionInfo":{"status":"ok","timestamp":1708933681763,"user_tz":-210,"elapsed":357305,"user":{"displayName":"Hosein Nekouei","userId":"03847242090053515103"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25021 files belonging to 2 classes.\n","Shape of inputs: (32,)\n","dtype of inputs: <dtype: 'string'>\n","taeger[0]: 0\n","Found 25001 files belonging to 2 classes.\n","Shape of inputs: (32,)\n","dtype of inputs: <dtype: 'string'>\n","taeger[0]: 0\n"]}],"source":["TRAIN_DS_PATH= '/content/drive/MyDrive/dataset/aclImdb/train'\n","TEST_DS_PATH = '/content/drive/MyDrive/dataset/aclImdb/test'\n","\n","train_ds = load_text_dataset(directory_path= TRAIN_DS_PATH, batch_size= BATCH_SIZE)\n","\n","test_ds = load_text_dataset(directory_path= TEST_DS_PATH, batch_size= BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQmq_NR3Z4DQ"},"outputs":[],"source":["int_train_ds, int_test_ds = proprocess_text_data(\n","                                                train_ds = train_ds,\n","                                                test_ds = test_ds,\n","                                                max_token= MAX_TOKENS,\n","                                                output_mode= 'int',\n","                                                output_sequence_length= MAX_LENGTH )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmId6fpS6lDX"},"outputs":[],"source":["history = gru_model(max_token= MAX_TOKENS)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyPGJ5yQSrS9n7jaRyG5oVH8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}